{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'auto_gptq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/animeshsengupta/Work_Directory/DACSS/SEM3/CS682/project/local_langchain.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/animeshsengupta/Work_Directory/DACSS/SEM3/CS682/project/local_langchain.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mauto_gptq\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoGPTQForCausalLM, BaseQuantizeConfig\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/animeshsengupta/Work_Directory/DACSS/SEM3/CS682/project/local_langchain.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_config\u001b[39m(has_desc_act):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/animeshsengupta/Work_Directory/DACSS/SEM3/CS682/project/local_langchain.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m BaseQuantizeConfig(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/animeshsengupta/Work_Directory/DACSS/SEM3/CS682/project/local_langchain.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         bits\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,  \u001b[39m# quantize model to 4-bit\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/animeshsengupta/Work_Directory/DACSS/SEM3/CS682/project/local_langchain.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         group_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m,  \u001b[39m# it is recommended to set the value to 128\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/animeshsengupta/Work_Directory/DACSS/SEM3/CS682/project/local_langchain.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         desc_act\u001b[39m=\u001b[39mhas_desc_act\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/animeshsengupta/Work_Directory/DACSS/SEM3/CS682/project/local_langchain.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'auto_gptq'"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "def get_config(has_desc_act):\n",
    "    return BaseQuantizeConfig(\n",
    "        bits=4,  # quantize model to 4-bit\n",
    "        group_size=128,  # it is recommended to set the value to 128\n",
    "        desc_act=has_desc_act\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4aba066909e44cfafaf2da5d126a279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965da15db155456dbd1e990921a5c380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519b2bffb3594cbd9e8a912cdf9737cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d477cc4f76495a84ea881ce94fa8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a6e3ffc7c64017bf1e7bcc75a67b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/874 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "GPU is required to quantize or run quantize model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/animeshsengupta/Work_Directory/DACSS/SEM3/CS682/project/local_langchain.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/animeshsengupta/Work_Directory/DACSS/SEM3/CS682/project/local_langchain.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTheBloke/vicuna-7B-v1.5-GPTQ\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/animeshsengupta/Work_Directory/DACSS/SEM3/CS682/project/local_langchain.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/animeshsengupta/Work_Directory/DACSS/SEM3/CS682/project/local_langchain.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name,use_safetensors\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmps\u001b[39;49m\u001b[39m\"\u001b[39;49m,)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/modeling_utils.py:2570\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2566\u001b[0m     quantization_method_from_args \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mGPTQ\n\u001b[1;32m   2567\u001b[0m     \u001b[39mor\u001b[39;00m quantization_method_from_config \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mGPTQ\n\u001b[1;32m   2568\u001b[0m ):\n\u001b[1;32m   2569\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m-> 2570\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGPU is required to quantize or run quantize model.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2571\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_optimum_available() \u001b[39mand\u001b[39;00m is_auto_gptq_available()):\n\u001b[1;32m   2572\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   2573\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLoading GPTQ quantized model requires optimum library : `pip install optimum` and auto-gptq library \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpip install auto-gptq\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2574\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: GPU is required to quantize or run quantize model."
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name=\"TheBloke/vicuna-7B-v1.5-GPTQ\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,use_safetensors=True,device=\"mps\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_parameters():\n",
    "    return {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.0001,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"top_k\": 1,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True\n",
    "        #\"stopping_strings\": [STOP_TOKEN + \":\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=500)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "template1 = \"\"\" You are an expert diplomat and a professor of diplomacy and internation relations. You are a confident teacher and often persuade other to believe in your opinions. \n",
    "To answer your questions you provide atleast 5 unique points explaining your reason in a step by step process.\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt1 = PromptTemplate(template=template1, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain1 = LLMChain(prompt=prompt1, \n",
    "                     llm=hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = \"\"\" You are a very curious learner who asks questions to understand others opinion better. You reason from others opinion and answer 1 new question about the opinion each time to understand the topic better.\n",
    "\n",
    "Opinion: {opinion}\n",
    "Your question:\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = PromptTemplate(template=template2, input_variables=[\"opinion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain2=LLMChain(prompt=prompt2, \n",
    "                     llm=hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=llm_chain2.run(opinion)\n",
    "print(out) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
